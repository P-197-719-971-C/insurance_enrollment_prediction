<!DOCTYPE html>
<html>
<head>
<title>report.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="insurance-enrollment-prediction-project-report">Insurance Enrollment Prediction Project Report</h1>
<h2 id="project-overview">Project Overview</h2>
<p>This project focuses on predicting whether employees will enroll in an insurance plan based on their demographic and employment information. The goal was to build a reliable machine learning model that could help the HR team identify employees likely to enroll, enabling better resource planning and targeted outreach.</p>
<p>The dataset contains 10,000 employee records with 10 features including age, salary, tenure, employment type, and other demographic information. The target variable is binary: 1 for enrolled and 0 for not enrolled.</p>
<hr>
<h2 id="exploratory-data-analysis">Exploratory Data Analysis</h2>
<h3 id="data-quality-assessment">Data Quality Assessment</h3>
<p>Before diving into the analysis, I checked the data for any quality issues. The dataset turned out to be pretty clean with no missing values and zero duplicate rows. This saved a lot of preprocessing time and meant I could focus directly on understanding the relationships in the data.</p>
<p>The dataset has a slight class imbalance with 61.74% of employees enrolled and 38.26% not enrolled. It's not severe enough to require special handling, but I kept this in mind when evaluating model performance.</p>
<h3 id="numerical-features-analysis">Numerical Features Analysis</h3>
<p>I analyzed the three numerical features: Age, Salary, and Tenure Years.</p>
<p><strong>Salary</strong> showed the strongest relationship with enrollment. Employees who enrolled had an average salary of around $69,337 compared to $58,087 for those who didn't enroll. The correlation coefficient of 0.37 indicates a moderate positive relationship, meaning higher salary = more likely to enroll. This makes intuitive sense since employees with higher incomes can more comfortably afford insurance premiums.</p>
<p><img src="file:///Users/umrav/Desktop/workspace/Job Assignment projects/Uniblox/insurance-enrollment-prediction/notebooks/numerical_plots/salary_distribution.png" alt="Salary Distribution"></p>
<p><strong>Age</strong> also showed a positive relationship with enrollment. The average age of enrolled employees was 45.6 years compared to 38.8 years for non-enrolled. This suggests older employees tend to be more interested in insurance coverage, possibly due to health concerns increasing with age.</p>
<p><img src="file:///Users/umrav/Desktop/workspace/Job Assignment projects/Uniblox/insurance-enrollment-prediction/notebooks/numerical_plots/age_distribution.png" alt="Age Distribution"></p>
<p><strong>Tenure Years</strong> was surprisingly not meaningful. The correlation was essentially zero (-0.01), indicating that how long someone has been with the company doesn't affect their enrollment decision.</p>
<p><img src="file:///Users/umrav/Desktop/workspace/Job Assignment projects/Uniblox/insurance-enrollment-prediction/notebooks/correlation.png" alt="Numerical Features Correlation"></p>
<h3 id="categorical-features-analysis">Categorical Features Analysis</h3>
<p>For categorical variables, I used Chi-square tests and Cramér's V to measure associations since these are more appropriate for categorical data.</p>
<p><strong>Has Dependents</strong> was the strongest categorical predictor. Employees with dependents had a 79.7% enrollment rate compared to just 34.8% for those without. That's a huge difference! Having family responsibilities clearly motivates people to get insurance coverage.</p>
<p><img src="file:///Users/umrav/Desktop/workspace/Job Assignment projects/Uniblox/insurance-enrollment-prediction/notebooks/categorical_plots/has_dependents_analysis.png" alt="Has Dependents Analysis"></p>
<p><strong>Employment Type</strong> was another strong predictor. Full-time employees had a 75.3% enrollment rate, while contract employees only enrolled at 31.2% and part-time at 28.5%. This shows that job stability and presumably income stability play a big role in enrollment decisions.</p>
<p><img src="file:///Users/umrav/Desktop/workspace/Job Assignment projects/Uniblox/insurance-enrollment-prediction/notebooks/categorical_plots/employment_type_analysis.png" alt="Employment Type Analysis"></p>
<p>Interestingly, <strong>Gender</strong>, <strong>Marital Status</strong>, and <strong>Region</strong> all showed very weak associations with enrollment. These demographic factors don't seem to influence the decision much.</p>
<p><img src="file:///Users/umrav/Desktop/workspace/Job Assignment projects/Uniblox/insurance-enrollment-prediction/notebooks/cramers_v.png" alt="Categorical Features Analysis"></p>
<h3 id="feature-importance-ranking">Feature Importance Ranking</h3>
<p>Using mutual information (which captures both linear and non-linear relationships), I ranked the features:</p>
<p><strong>Top Predictors:</strong></p>
<ol>
<li>Has Dependents</li>
<li>Employment Type</li>
<li>Salary</li>
<li>Age</li>
</ol>
<p><strong>Low Impact:</strong></p>
<ul>
<li>Tenure Years</li>
<li>Gender</li>
<li>Marital Status</li>
<li>Region</li>
</ul>
<p><img src="file:///Users/umrav/Desktop/workspace/Job Assignment projects/Uniblox/insurance-enrollment-prediction/notebooks/mutual_information.png" alt="Mutual Information"></p>
<h3 id="key-takeaways-from-eda">Key Takeaways from EDA</h3>
<p>The main drivers of enrollment are:</p>
<ul>
<li><strong>Financial capacity</strong> (salary)</li>
<li><strong>Family responsibilities</strong> (having dependents)</li>
<li><strong>Job stability</strong> (full-time employment)</li>
<li><strong>Age</strong> (older employees enroll more)</li>
</ul>
<p>This all makes practical sense. People with families, stable jobs, and higher incomes are more likely to prioritize getting insurance.</p>
<hr>
<h2 id="technical-implementation">Technical Implementation</h2>
<h3 id="model-selection">Model Selection</h3>
<p>I implemented three different models to compare performance:</p>
<ol>
<li><strong>Logistic Regression</strong> - A simple, interpretable baseline model (capture linear relationship)</li>
<li><strong>Random Forest</strong> - An ensemble method that handles non-linear relationships</li>
<li><strong>LightGBM</strong> - A gradient boosting method known for speed and accuracy(works best with large datasets)</li>
</ol>
<h3 id="preprocessing-steps">Preprocessing Steps</h3>
<p>The preprocessing pipeline included:</p>
<ul>
<li>Encoding categorical variables using one-hot encoding for logistic regression</li>
<li>Encoding categorical variables using ordinal encoding for random forest and lightgbm</li>
<li>Scaling numerical features using StandardScaler for logistic regression</li>
<li>Did not scale numerical features for random forest and lightgbm (inherently handles it)</li>
<li>Splitting data into 80% training and 20% testing</li>
<li>Using stratified sampling to maintain class balance</li>
</ul>
<h3 id="hyperparameter-tuning">Hyperparameter Tuning</h3>
<p>For each model, I used GridSearchCV to find the best hyperparameters:</p>
<p><strong>Logistic Regression:</strong></p>
<ul>
<li>Regularization strength (C): [0.1, 1, 10]</li>
<li>Solver: ['lbfgs', 'liblinear']</li>
</ul>
<p><strong>Random Forest:</strong></p>
<ul>
<li>Number of estimators: [100, 200]</li>
<li>Max depth: [10, 20, None]</li>
<li>Min samples split: [2, 5]</li>
</ul>
<p><strong>LightGBM:</strong></p>
<ul>
<li>Number of estimators: [100, 200]</li>
<li>Learning rate: [0.05, 0.1]</li>
<li>Max depth: [5, 10]</li>
</ul>
<h3 id="model-performance">Model Performance</h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>Precision</th>
<th>Recall</th>
<th>F1-Score</th>
<th>ROC-AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td>Logistic Regression</td>
<td>89.7%</td>
<td>91.0%</td>
<td>92.4%</td>
<td>91.7%</td>
<td>97.1%</td>
</tr>
<tr>
<td>Random Forest</td>
<td>99.9%</td>
<td>100%</td>
<td>99.9%</td>
<td>99.9%</td>
<td>100%</td>
</tr>
<tr>
<td>LightGBM</td>
<td>99.9%</td>
<td>100%</td>
<td>99.9%</td>
<td>99.9%</td>
<td>100%</td>
</tr>
</tbody>
</table>
<p>Both Random Forest and LightGBM achieved near-perfect performance on the test set. The Logistic Regression baseline performed reasonably well at around 90% accuracy but couldn't capture the complex relationships but since our data is synthetic and is more generalized, logistic regresion is best choice, also its explainable.</p>
<h3 id="overfitting-check">Overfitting Check</h3>
<p>I compared training and test metrics to ensure the models weren't overfitting:</p>
<ul>
<li><strong>Logistic Regression:</strong> Train AUC 96.7% vs Test AUC 97.1% - No overfitting</li>
<li><strong>Random Forest:</strong> Train AUC 100% vs Test AUC 100% - No overfitting</li>
<li><strong>LightGBM:</strong> Train AUC 100% vs Test AUC 100% - No overfitting</li>
</ul>
<p>All models generalize well to unseen data.</p>
<hr>
<h2 id="deployment--interfaces">Deployment &amp; Interfaces</h2>
<p>I've built two user-friendly interfaces to make the model accessible for real-world use.</p>
<h3 id="fastapi-rest-api">FastAPI REST API</h3>
<p>The FastAPI backend provides programmatic access to the prediction models. It supports both single and batch predictions with detailed response information including probability scores and confidence levels.</p>
<p><strong>API Endpoints:</strong></p>
<table>
<thead>
<tr>
<th>Endpoint</th>
<th>Method</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>/health</code></td>
<td>GET</td>
<td>System health check and model status</td>
</tr>
<tr>
<td><code>/models</code></td>
<td>GET</td>
<td>List available models</td>
</tr>
<tr>
<td><code>/model/info</code></td>
<td>GET</td>
<td>Get detailed model information</td>
</tr>
<tr>
<td><code>/predict</code></td>
<td>POST</td>
<td>Single employee prediction</td>
</tr>
<tr>
<td><code>/predict/batch</code></td>
<td>POST</td>
<td>Batch predictions (up to 1000)</td>
</tr>
<tr>
<td><code>/predict/explain</code></td>
<td>POST</td>
<td>Prediction with feature importance</td>
</tr>
</tbody>
</table>
<p><strong>Running the API:</strong></p>
<pre class="hljs"><code><div>uv run uvicorn api:app --reload
</div></code></pre>
<p>The API will be available at <code>http://localhost:8000</code> with interactive documentation at <code>/docs</code>.</p>
<h3 id="gradio-web-dashboard">Gradio Web Dashboard</h3>
<p>I also built a user-friendly web interface using Gradio that doesn't require any coding knowledge to use. HR staff can easily input employee data and get predictions through a clean visual interface.</p>
<p><strong>Dashboard Features:</strong></p>
<ul>
<li><strong>Single Prediction Tab</strong> - Enter employee details and get instant predictions</li>
<li><strong>Batch Prediction Tab</strong> - Upload CSV/Excel files for bulk predictions</li>
<li><strong>History Tab</strong> - View all past predictions with export options (Excel, CSV, PDF)</li>
<li><strong>Model Comparison</strong> - Try the same employee data across all three models</li>
</ul>
<p><strong>Running the Dashboard:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># First, start the API server</span>
uv run uvicorn api:app --reload

<span class="hljs-comment"># Then launch the Gradio interface</span>
uv run python gradio_app.py
</div></code></pre>
<p>The dashboard opens at <code>http://localhost:7860</code>.</p>
<h3 id="docker-deployment">Docker Deployment</h3>
<p>For easy deployment, the entire application is containerized using Docker:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Build the image</span>
docker build -t insurance-prediction .

<span class="hljs-comment"># Run the container</span>
docker run -p 8000:8000 insurance-prediction
</div></code></pre>
<p>The Docker setup includes both the FastAPI server and can be configured to run the Gradio dashboard as well.</p>
<hr>
<hr>
<h2 id="conclusions--what-id-do-next">Conclusions &amp; What I’d Do Next</h2>
<p>The analysis showed clear patterns in employee insurance enrollment. The strongest factors were having dependents, employment type, salary, and age, while features like gender and marital status had minimal impact.</p>
<p>Random Forest and LightGBM achieved very high performance on the synthetic dataset, while Logistic Regression provided a solid and interpretable baseline. Since the dataset is synthetic, I would be cautious about relying purely on these results without validating on real-world data.</p>
<h3 id="what-id-do-next">What I’d Do Next</h3>
<p>If given more time, I would focus on improving validation and preparing the model for real-world deployment:</p>
<ul>
<li>
<p><strong>Validate on real data:</strong> Test the model on new employee cohorts to ensure it generalizes beyond synthetic data.</p>
</li>
<li>
<p><strong>Data Enhancement:</strong> Collect additional relevant features (e.g., prior insurance coverage, claim history, benefit usage patterns) to improve predictive power.</p>
</li>
<li>
<p><strong>Enhance features:</strong> Explore interaction features (e.g., salary × employment type) and incorporate additional business-relevant attributes if available.</p>
</li>
<li>
<p><strong>Refine the model:</strong> Continue systematic hyperparameter tuning and evaluate whether more complex models meaningfully outperform simpler, interpretable ones.</p>
</li>
<li>
<p><strong>Calibrate probabilities:</strong> Tune decision thresholds based on business objectives (e.g., prioritizing recall for outreach campaigns).</p>
</li>
<li>
<p><strong>Production readiness:</strong> Implement monitoring for data drift and define a retraining strategy to maintain long-term performance.</p>
</li>
<li>
<p><strong>Optional LLM use:</strong> If unstructured HR data becomes available (e.g., feedback or notes), LLMs could be explored for feature extraction.</p>
</li>
</ul>
<p>Overall, the next steps would focus less on squeezing out marginal accuracy gains and more on reliability, interpretability, and real-world impact.</p>

</body>
</html>
